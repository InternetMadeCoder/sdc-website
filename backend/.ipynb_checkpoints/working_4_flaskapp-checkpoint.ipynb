{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001aa2ec-1724-463c-821a-9dc0fc909ba3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Simple tokenization function (no NLTK dependency)\n",
    "def tokenize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    # Split by whitespace\n",
    "    return [token.strip() for token in text.split() if token.strip()]\n",
    "\n",
    "# Simple stemming function (for handling plurals without NLTK)\n",
    "def simple_stem(word):\n",
    "    \"\"\"Very basic stemming for English words\"\"\"\n",
    "    if not word or not isinstance(word, str):\n",
    "        return word\n",
    "        \n",
    "    word = word.lower()\n",
    "    \n",
    "    # Common plural endings\n",
    "    if len(word) > 3:\n",
    "        if word.endswith('ies') and len(word) > 4:\n",
    "            return word[:-3] + 'y'  # universities -> university\n",
    "        elif word.endswith('es') and len(word) > 3:\n",
    "            return word[:-2]  # classes -> class\n",
    "        elif word.endswith('s') and not word.endswith('ss'):\n",
    "            return word[:-1]  # students -> student\n",
    "    \n",
    "    return word\n",
    "\n",
    "# Text preprocessing function (NLTK-free)\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Tokenize\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # Apply simple stemming\n",
    "    stemmed_tokens = [simple_stem(token) for token in tokens]\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def extract_important_terms(text):\n",
    "    \"\"\"Extract important keywords from text that should be mandatory for matching.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # List of common words that aren't important for matching\n",
    "    common_words = {\n",
    "        'where', 'what', 'when', 'how', 'who', 'why', \n",
    "        'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "        'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to', 'from', 'by',\n",
    "        'about', 'like', 'through', 'over', 'before', 'between', 'after',\n",
    "        'since', 'without', 'under', 'within', 'along', 'following',\n",
    "        'can', 'could', 'should', 'would', 'may', 'might', 'must',\n",
    "        'do', 'does', 'did', 'doing'\n",
    "    }\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    words = [w.strip() for w in text.split() if w.strip()]\n",
    "    \n",
    "    # Extract potentially important terms (not in common_words)\n",
    "    important_terms = [w for w in words if w not in common_words]\n",
    "    \n",
    "    # Extract potential proper nouns (capitalized words in original text)\n",
    "    original_words = text.split()\n",
    "    proper_nouns = []\n",
    "    for word in original_words:\n",
    "        if word and word[0].isupper():\n",
    "            proper_nouns.append(word.lower())\n",
    "    \n",
    "    # Prioritize proper nouns and multi-word terms\n",
    "    priority_terms = list(set(proper_nouns))\n",
    "    \n",
    "    # Add any terms that might be locations or specific identifiers\n",
    "    # (numbers, building names, etc.)\n",
    "    specific_indicators = [\n",
    "        w for w in important_terms \n",
    "        if any(char.isdigit() for char in w)  # Contains digits\n",
    "        or len(w) > 3  # Longer terms are often more specific\n",
    "    ]\n",
    "    \n",
    "    priority_terms.extend(specific_indicators)\n",
    "    \n",
    "    return list(set(priority_terms))\n",
    "\n",
    "# Load and clean data\n",
    "file_path = 'university_faq.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['Processed_Question'] = df['Question'].apply(preprocess_text)\n",
    "df['Processed_Keyword'] = df['Keyword'].apply(preprocess_text)\n",
    "df['Important_Terms'] = df['Question'].apply(extract_important_terms)\n",
    "\n",
    "# Add entity extraction to keywords too\n",
    "df['Keyword_Entities'] = df['Keyword'].apply(extract_important_terms)\n",
    "\n",
    "# Build domain dictionary for spelling correction\n",
    "domain_dictionary = set()\n",
    "for text in df['Question'].tolist() + df['Keyword'].tolist():\n",
    "    if isinstance(text, str):\n",
    "        tokens = tokenize(text)\n",
    "        domain_dictionary.update(tokens)\n",
    "\n",
    "# Display first few rows of the processed dataset\n",
    "df.head()\n",
    "\n",
    "# Function to generate variations\n",
    "def generate_variations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return [text] if text else [\"\"]\n",
    "        \n",
    "    variations = [text]\n",
    "    \n",
    "    # Simple plural/singular handling\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        # Try both stemmed and original forms\n",
    "        stemmed = simple_stem(word)\n",
    "        if stemmed != word:\n",
    "            new_words = words.copy()\n",
    "            new_words[i] = stemmed\n",
    "            variations.append(' '.join(new_words))\n",
    "    \n",
    "    # Add common misspellings for university terms\n",
    "    common_misspellings = {\n",
    "        'university': ['univercity', 'univarsity', 'uni'],\n",
    "        'admission': ['admision', 'admisssion'],\n",
    "        'scholarship': ['scolarship', 'scholarshipp'],\n",
    "        'registration': ['registraton', 'registeration', 'signup'],\n",
    "        'course': ['cours', 'coarse', 'class'],\n",
    "        'professor': ['professer', 'proffesor', 'prof'],\n",
    "        'semester': ['semister', 'semestre', 'term'],\n",
    "        'tuition': ['tution', 'tuishon', 'fees'],\n",
    "        'degree': ['degre', 'diploma', 'qualification'],\n",
    "        'dormitory': ['dorm', 'housing', 'residence'],\n",
    "        'major': ['specialization', 'concentration', 'field'],\n",
    "        'credit': ['cred', 'unit', 'point'],\n",
    "        'exam': ['examination', 'test', 'assessment'],\n",
    "        'financial': ['fiscal', 'money', 'monetary'],\n",
    "        'transfer': ['xfer', 'switch', 'change'],\n",
    "        'deadline': ['due date', 'cutoff', 'timeframe'],\n",
    "        'application': ['app', 'apply', 'submission'],\n",
    "        'schedule': ['timetable', 'calendar', 'agenda']\n",
    "    }\n",
    "    \n",
    "    for word, misspellings in common_misspellings.items():\n",
    "        if word in text:\n",
    "            for misspelling in misspellings:\n",
    "                variations.append(text.replace(word, misspelling))\n",
    "                \n",
    "    return variations\n",
    "\n",
    "# Uncommenting this section will expand your dataset with variations\n",
    "\n",
    "# Option to expand dataset with variations\n",
    "expanded_questions = []\n",
    "expanded_keywords = []\n",
    "expanded_answers = []\n",
    "expanded_indices = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    question_variations = generate_variations(row['Processed_Question'])\n",
    "    keyword_variations = generate_variations(row['Processed_Keyword'])\n",
    "    \n",
    "    for q_var in question_variations:\n",
    "        for k_var in keyword_variations:\n",
    "            expanded_questions.append(q_var)\n",
    "            expanded_keywords.append(k_var)\n",
    "            expanded_answers.append(row['Answer'])\n",
    "            expanded_indices.append(idx)  # Keep track of original index\n",
    "\n",
    "expanded_df = pd.DataFrame({\n",
    "    'Processed_Question': expanded_questions,\n",
    "    'Processed_Keyword': expanded_keywords,\n",
    "    'Answer': expanded_answers,\n",
    "    'Original_Index': expanded_indices\n",
    "})\n",
    "\n",
    "# Use expanded dataset or original\n",
    "# use_df = expanded_df\n",
    "use_df = df  # Comment this out if using expanded dataset\n",
    "\n",
    "\n",
    "# Initialize TF-IDF vectorizers with improved parameters\n",
    "question_vectorizer = TfidfVectorizer(\n",
    "    min_df=1, max_df=0.9,\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "keyword_vectorizer = TfidfVectorizer(\n",
    "    min_df=1, max_df=0.9,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit vectorizers\n",
    "question_vectors = question_vectorizer.fit_transform(df['Processed_Question'])\n",
    "keyword_vectors = keyword_vectorizer.fit_transform(df['Processed_Keyword'])\n",
    "\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) if text else \"\"\n",
    "        \n",
    "    try:\n",
    "        corrected = str(TextBlob(text).correct())\n",
    "        \n",
    "        # Protect domain-specific terms from \"correction\"\n",
    "        words = text.lower().split()\n",
    "        corrected_words = corrected.lower().split()\n",
    "        \n",
    "        # Only replace words if length matches\n",
    "        if len(words) == len(corrected_words):\n",
    "            for i, (orig_word, corr_word) in enumerate(zip(words, corrected_words)):\n",
    "                if orig_word in domain_dictionary and orig_word != corr_word:\n",
    "                    corrected_words[i] = orig_word\n",
    "            \n",
    "            return ' '.join(corrected_words)\n",
    "        return corrected\n",
    "    except Exception as e:\n",
    "        print(f\"Spelling correction error: {e}\")\n",
    "        return text\n",
    "\n",
    "# Matching function\n",
    "def match_input(processed_input, original_input):\n",
    "    try:\n",
    "        # Extract important terms from user input\n",
    "        user_important_terms = extract_important_terms(original_input)\n",
    "        \n",
    "        # Vector matching\n",
    "        user_question_vector = question_vectorizer.transform([processed_input])\n",
    "        question_similarities = cosine_similarity(user_question_vector, question_vectors)\n",
    "        \n",
    "        # Get top 5 matches for more detailed analysis\n",
    "        top_n = 5\n",
    "        top_question_indices = question_similarities[0].argsort()[-top_n:][::-1]\n",
    "        top_question_scores = question_similarities[0][top_question_indices]\n",
    "        \n",
    "        # Do the same for keywords\n",
    "        user_keyword_vector = keyword_vectorizer.transform([processed_input])\n",
    "        keyword_similarities = cosine_similarity(user_keyword_vector, keyword_vectors)\n",
    "        top_keyword_indices = keyword_similarities[0].argsort()[-top_n:][::-1]\n",
    "        top_keyword_scores = keyword_similarities[0][top_keyword_indices]\n",
    "        \n",
    "        # Check for important term matches in top question matches\n",
    "        question_match_idx = -1\n",
    "        question_score = 0\n",
    "        \n",
    "        for i, idx in enumerate(top_question_indices):\n",
    "            db_important_terms = df.iloc[idx]['Important_Terms']\n",
    "            \n",
    "            # Calculate what percentage of important terms from the DB question\n",
    "            # are found in the user query\n",
    "            matching_terms = [term for term in db_important_terms if term in processed_input]\n",
    "            \n",
    "            # Only consider it a match if at least one important term matches\n",
    "            if len(matching_terms) > 0:\n",
    "                # Adjust score based on important term matches\n",
    "                term_match_ratio = len(matching_terms) / max(1, len(db_important_terms))\n",
    "                adjusted_score = top_question_scores[i] * (0.5 + 0.5 * term_match_ratio)\n",
    "                \n",
    "                if adjusted_score > question_score:\n",
    "                    question_score = adjusted_score\n",
    "                    question_match_idx = idx\n",
    "        \n",
    "        # Check for important term matches in top keyword matches\n",
    "        keyword_match_idx = -1\n",
    "        keyword_score = 0\n",
    "        \n",
    "        for i, idx in enumerate(top_keyword_indices):\n",
    "            keyword_entities = df.iloc[idx]['Keyword_Entities']\n",
    "            \n",
    "            # Check if any keyword entities match\n",
    "            matching_entities = [entity for entity in keyword_entities if entity in processed_input]\n",
    "            \n",
    "            # Only consider it a match if at least one entity matches\n",
    "            if len(matching_entities) > 0:\n",
    "                # Adjust score based on entity matches\n",
    "                entity_match_ratio = len(matching_entities) / max(1, len(keyword_entities))\n",
    "                adjusted_score = top_keyword_scores[i] * (0.5 + 0.5 * entity_match_ratio)\n",
    "                \n",
    "                if adjusted_score > keyword_score:\n",
    "                    keyword_score = adjusted_score\n",
    "                    keyword_match_idx = idx\n",
    "        \n",
    "        # If we didn't find matches with important terms, fall back to the original method\n",
    "        # but with reduced confidence\n",
    "        if question_match_idx == -1:\n",
    "            question_match_idx = top_question_indices[0]\n",
    "            question_score = top_question_scores[0] * 0.7  # Reduce confidence\n",
    "            \n",
    "        if keyword_match_idx == -1:\n",
    "            keyword_match_idx = top_keyword_indices[0]\n",
    "            keyword_score = top_keyword_scores[0] * 0.7  # Reduce confidence\n",
    "        \n",
    "        # Determine best match\n",
    "        if keyword_score > question_score:\n",
    "            return {\n",
    "                'answer': df.iloc[keyword_match_idx]['Answer'],\n",
    "                'confidence': keyword_score,\n",
    "                'match_type': 'keyword',\n",
    "                'matched_question': df.iloc[keyword_match_idx]['Question']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'answer': df.iloc[question_match_idx]['Answer'],\n",
    "                'confidence': question_score,\n",
    "                'match_type': 'question', \n",
    "                'matched_question': df.iloc[question_match_idx]['Question']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Vector matching error: {e}\")\n",
    "        return {'answer': \"\", 'confidence': 0.0, 'match_type': 'error', 'matched_question': \"\"}\n",
    "\n",
    "def fuzzy_match(user_input):\n",
    "    max_score = 0\n",
    "    best_idx = -1\n",
    "    \n",
    "    # Extract important terms from user input\n",
    "    user_important_terms = extract_important_terms(user_input)\n",
    "    \n",
    "    # Try both question and keyword fuzzy matching\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            question = row.get('Question', '')\n",
    "            keyword = row.get('Keyword', '')\n",
    "            \n",
    "            if isinstance(question, str) and isinstance(keyword, str):\n",
    "                # Use token_sort_ratio for better handling of word order differences\n",
    "                q_score = fuzz.token_sort_ratio(user_input.lower(), question.lower())\n",
    "                k_score = fuzz.token_sort_ratio(user_input.lower(), keyword.lower())\n",
    "                \n",
    "                # Also try partial ratio for substring matching\n",
    "                q_partial = fuzz.partial_ratio(user_input.lower(), question.lower())\n",
    "                k_partial = fuzz.partial_ratio(user_input.lower(), keyword.lower())\n",
    "                \n",
    "                # Take the best score\n",
    "                max_row_score = max(q_score, k_score, q_partial, k_partial) / 100\n",
    "                \n",
    "                # Check if important terms match\n",
    "                important_terms = row.get('Important_Terms', [])\n",
    "                if important_terms:\n",
    "                    matching_terms = [term for term in important_terms if term in user_input.lower()]\n",
    "                    # If no important terms match, reduce the score\n",
    "                    if len(matching_terms) == 0 and len(important_terms) > 0:\n",
    "                        max_row_score *= 0.7\n",
    "                \n",
    "                if max_row_score > max_score:\n",
    "                    max_score = max_row_score\n",
    "                    best_idx = idx\n",
    "        except Exception as e:\n",
    "            print(f\"Fuzzy matching error on row {idx}: {e}\")\n",
    "    \n",
    "    if best_idx >= 0:\n",
    "        return {\n",
    "            'answer': df.iloc[best_idx]['Answer'],\n",
    "            'confidence': max_score,\n",
    "            'match_type': 'fuzzy',\n",
    "            'matched_question': df.iloc[best_idx]['Question']\n",
    "        }\n",
    "    return {'answer': \"\", 'confidence': 0.0, 'match_type': 'fuzzy_failed', 'matched_question': \"\"}\n",
    "\n",
    "# Enhanced response function\n",
    "def get_response(user_input, confidence_threshold=0.5):\n",
    "    if not isinstance(user_input, str) or not user_input.strip():\n",
    "        return \"I need a question to help you.\", 0.0\n",
    "    \n",
    "    # Step 1: Preprocess input\n",
    "    processed_input = preprocess_text(user_input)\n",
    "    \n",
    "    # Step 2: Try with original preprocessed input\n",
    "    result = match_input(processed_input, user_input)\n",
    "    if result['confidence'] >= confidence_threshold:\n",
    "        return result['answer'], result['confidence'], result['matched_question']\n",
    "    \n",
    "    # Step 3: Try spelling correction if confidence is low\n",
    "    corrected_input = correct_spelling(user_input)\n",
    "    if corrected_input.lower() != user_input.lower():\n",
    "        processed_corrected = preprocess_text(corrected_input)\n",
    "        corrected_result = match_input(processed_corrected, corrected_input)\n",
    "        if corrected_result['confidence'] > result['confidence']:\n",
    "            return corrected_result['answer'], corrected_result['confidence'], corrected_result['matched_question'], corrected_input\n",
    "    \n",
    "    # Step 4: Try fuzzy matching as another fallback\n",
    "    fuzzy_result = fuzzy_match(user_input)\n",
    "    if fuzzy_result['confidence'] > result['confidence']:\n",
    "        return fuzzy_result['answer'], fuzzy_result['confidence'], fuzzy_result.get('matched_question', '')\n",
    "    \n",
    "    # Return best result found, even if confidence is low\n",
    "    if result['confidence'] > 0.15:  # Minimum threshold\n",
    "        return result['answer'], result['confidence'], result['matched_question']\n",
    "    \n",
    "    return \"I'm not sure I understand. Could you rephrase your question?\", 0.0, \"\"\n",
    "\n",
    "# Example usage\n",
    "def chatbot_response(user_input):\n",
    "    try:\n",
    "        result = get_response(user_input)\n",
    "        \n",
    "        # Unpack response based on length\n",
    "        if len(result) == 4:  # Corrected spelling case\n",
    "            answer, confidence, matched_question, corrected = result\n",
    "            response_text = \"\"\n",
    "            \n",
    "            if confidence < 0.4:\n",
    "                response_text = f\"Did you mean: '{corrected}'?\\n\\nI need to research this further. Would you like me to send your question to our faculty? Please provide your email address.\"\n",
    "            elif confidence > 0.6:\n",
    "                response_text = f\"Did you mean: '{corrected}'?\\n\\n{answer}\"\n",
    "            else:\n",
    "                response_text = f\"Did you mean: '{corrected}'?\\n\\nI think you're asking about: '{matched_question}'\\n\\n{answer}\"\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        elif len(result) == 3:  # Normal case with matched question\n",
    "            answer, confidence, matched_question = result\n",
    "            \n",
    "            if confidence < 0.4:\n",
    "                return \"I need to research this further. Would you like me to send your question to our faculty? Please provide your email address.\"\n",
    "            elif confidence > 0.6:\n",
    "                return answer\n",
    "            else:\n",
    "                return f\"I think you're asking about: '{matched_question}'\\n\\n{answer}\"\n",
    "        else:\n",
    "            answer, confidence = result\n",
    "            if confidence < 0.4:\n",
    "                return \"I need to research this further. Would you like me to send your question to our faculty? Please provide your email address.\"\n",
    "            elif confidence > 0.6:\n",
    "                return answer\n",
    "            else:\n",
    "                return f\"I'm not entirely sure, but this might help: {answer}\"\n",
    "                \n",
    "    except Exception as e:\n",
    "        return f\"Sorry, I encountered an error processing your question. Please try again with different wording.\"\n",
    "\n",
    "# # Main chatbot loop\n",
    "# print(\"University FAQ Chatbot (type 'exit' to quit)\")\n",
    "# while True:\n",
    "#     user_input = input(\"Ask me something: \").strip()\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         print(\"Goodbye!\")\n",
    "#         break\n",
    "#     response = chatbot_response(user_input)\n",
    "#     print(f\"Chatbot: {response}\\n\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def test_and_visualize_questions(questions):\n",
    "#     results = []\n",
    "#     for q in questions:\n",
    "#         result = get_response(q)\n",
    "#         conf = result[1] if len(result) >= 2 else 0\n",
    "#         results.append({\n",
    "#             'question': q,\n",
    "#             'confidence': conf,\n",
    "#             'response': chatbot_response(q)\n",
    "#         })\n",
    "    \n",
    "#     # Create visualization\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "#     confidences = [r['confidence'] for r in results]\n",
    "#     questions = [r['question'] for r in results]\n",
    "    \n",
    "#     bar_colors = ['green' if c > 0.6 else 'orange' if c > 0.3 else 'red' for c in confidences]\n",
    "    \n",
    "#     bars = ax.bar(range(len(questions)), confidences, color=bar_colors)\n",
    "    \n",
    "#     ax.set_xlabel('Question')\n",
    "#     ax.set_ylabel('Confidence Score')\n",
    "#     ax.set_title('Chatbot Confidence Scores for Sample Questions')\n",
    "#     ax.set_xticks(range(len(questions)))\n",
    "#     ax.set_xticklabels(questions, rotation=45, ha='right')\n",
    "#     ax.set_ylim(0, 1)\n",
    "    \n",
    "#     # Add threshold lines\n",
    "#     ax.axhline(y=0.6, color='green', linestyle='--', alpha=0.5, label='High Confidence (0.6)')\n",
    "#     ax.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Medium Confidence (0.3)')\n",
    "    \n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Display detailed results in a table\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Test with sample questions\n",
    "# sample_questions = [\n",
    "#     \"CSE department programs?\",\n",
    "#     \"where is cheffies?\",\n",
    "#     # \"What kind of support does MUJ provide for CSE students interested in robotics research?\",\n",
    "#     \"scope of robotic in cse\",\n",
    "#     \"where is dome\",\n",
    "#     \"dome\",\n",
    "#     \"DoMe\",\n",
    "#     \"when will the admission start\",\n",
    "#     \"what food outlets are there\",\n",
    "#     \"When is the registration deadline?\",\n",
    "#     \"Whats the tuiton due date?\",  # Intentional typo\n",
    "#     \"Tell me about dormatories\",  # Intentional typo\n",
    "#     \"What random question that doesn't make sense?\",\n",
    "#     \"phsycology\"\n",
    "# ]\n",
    "\n",
    "# results_df = test_and_visualize_questions(sample_questions)\n",
    "# results_df\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
